{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import time\n",
    "from urllib.parse import urljoin, urlparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # URL of the page you want to scrape\n",
    "# url = 'https://www.example.com/user-agreement'\n",
    "\n",
    "# # Make a request to get the page content\n",
    "# response = requests.get(url)\n",
    "\n",
    "# # Parse the HTML content of the page\n",
    "# soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# # Identify the container of the user agreement section.\n",
    "# # This is a placeholder; you'll need to inspect the webpage to find the right selector\n",
    "# user_agreement_container = soup.find(id='userAgreementSection')\n",
    "\n",
    "# # If the container was found, print its content\n",
    "# if user_agreement_container:\n",
    "#     print(user_agreement_container.text)\n",
    "# else:\n",
    "#     print('User agreement section not found.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Load CSV ===\n",
    "df = pd.read_csv(\"company_list.csv\")  # Make sure this CSV has \"company\" and \"website\" columns\n",
    "urls = df[\"URL\"].dropna().unique()\n",
    "\n",
    "# === Output Directory ===\n",
    "os.makedirs(\"agreements\", exist_ok=True)\n",
    "\n",
    "# === Keyword list (case-insensitive search) ===\n",
    "keyword_list = [\n",
    "    \"privacy\", \"user agreement\", \"terms\", \"conditions\",\n",
    "    \"data use\", \"policy\", \"legal\", \"disclaimer\", \"statement\"\n",
    "]\n",
    "keyword_pattern = re.compile(\"|\".join(keywords), re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_relevant_link(text):\n",
    "    return any(kw in text.lower() for kw in keyword_list)\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "                  \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "                  \"Chrome/91.0.4472.124 Safari/537.36\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/22] Checking https://bushelpowered.com/\n",
      " - Found link: privacy notice → https://bushelpowered.com/privacy-notice\n",
      "   - Saved to agreements/0001.txt\n",
      "[2/22] Checking https://www.cibotechnologies.com/\n",
      " - Found link: privacy policy → http://www.cibotechnologies.com/privacy-policy/\n",
      "   - Saved to agreements/0002.txt\n",
      "[3/22] Checking https://agrevolution.in/\n",
      " - No relevant agreement links found.\n",
      "[4/22] Checking https://www.fbn.com/\n",
      " - Found link: terms of service → https://www.fbn.com/terms-of-service\n",
      "   - Saved to agreements/0003.txt\n",
      "[5/22] Checking https://www.indigoag.com/\n",
      " - Found link: privacy policy | → https://www.indigoag.com/privacy-policy?hsLang=en-us\n",
      "   - Saved to agreements/0004.txt\n",
      "[6/22] Checking https://producepay.com/\n",
      " - Skipping (HTTP 403)\n",
      "[7/22] Checking https://agrivida.com/\n",
      " - No relevant agreement links found.\n",
      "[8/22] Checking https://innovafeed.com/en/\n",
      " - Found link: legal notice → https://innovafeed.com/mentions-legales/\n",
      "   - Saved to agreements/0005.txt\n",
      "[9/22] Checking https://www.nativemicrobials.com/\n",
      " - Found link: privacy & terms → https://www.nativemicrobials.com/privacy-and-terms.html\n",
      "   - Saved to agreements/0006.txt\n",
      "[10/22] Checking https://protix.eu/\n",
      " - Found link: terms of use → https://protix.eu/terms-of-use\n",
      "   - Saved to agreements/0007.txt\n",
      "[11/22] Checking https://pure-salmon.com/\n",
      " - Skipping (HTTP 520)\n",
      "[12/22] Checking https://recombinetics.com/\n",
      " - No relevant agreement links found.\n",
      "[13/22] Checking https://www.ynsect.com/\n",
      " - Found link: privacy policy → https://www.ynsect.com/privacy-policy/\n",
      "   - Saved to agreements/0008.txt\n",
      "[14/22] Checking https://bowery.co/\n",
      " - Error accessing https://bowery.co/: HTTPSConnectionPool(host='bowery.co', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000002650EB3FDC0>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))\n",
      "[15/22] Checking https://www.infarm.com/\n",
      " - Error accessing https://www.infarm.com/: HTTPSConnectionPool(host='www.infarm.com', port=443): Max retries exceeded with url: / (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000002650EB3FEE0>: Failed to resolve 'www.infarm.com' ([Errno 11002] getaddrinfo failed)\"))\n",
      "[16/22] Checking https://www.plenty.ag/\n",
      " - Found link: privacy policy → https://www.plenty.ag/privacy-policy/\n",
      "   - Saved to agreements/0009.txt\n",
      "[17/22] Checking https://bioenergydevco.com/\n",
      " - Found link: privacy policy → https://live-bts-bioenergy.pantheonsite.io/wp-content/uploads/2025/04/PRIVACY-POLICY-MAY-2025.docx\n",
      " - Found link: terms and conditions → https://live-bts-bioenergy.pantheonsite.io/wp-content/uploads/2025/04/TERMS-OF-USE-MAY-2025.docx\n",
      " - No relevant agreement links found.\n",
      "[18/22] Checking https://www.concentricag.com/\n",
      " - Found link: terms of use → https://www.concentricag.com/terms-of-use\n",
      "   - Saved to agreements/0010.txt\n",
      "[19/22] Checking https://www.pivotbio.com/\n",
      " - Skipping (HTTP 403)\n",
      "[20/22] Checking https://www.cropnutrition.com/\n",
      " - Skipping (HTTP 403)\n",
      "[21/22] Checking https://www.sound.ag/\n",
      " - Found link: product terms → https://www.sound.ag/terms\n",
      "   - Saved to agreements/0011.txt\n",
      "[22/22] Checking https://agbiome.com/\n",
      " - Error accessing https://agbiome.com/: HTTPSConnectionPool(host='agbiome.com', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLCertVerificationError(1, \"[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: Hostname mismatch, certificate is not valid for 'agbiome.com'. (_ssl.c:1147)\")))\n",
      "\n",
      " Finished all sites.\n"
     ]
    }
   ],
   "source": [
    "file_index = 1\n",
    "\n",
    "for i, url in enumerate(urls):\n",
    "    print(f\"[{i+1}/{len(urls)}] Checking {url}\")\n",
    "    try:\n",
    "        response = requests.get(url, headers=HEADERS, timeout=10)\n",
    "        if response.status_code != 200:\n",
    "            print(f\" - Skipping (HTTP {response.status_code})\")\n",
    "            continue\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        links = soup.find_all('a', href=True)\n",
    "\n",
    "        found = False\n",
    "        for link in links:\n",
    "            link_text = link.get_text(strip=True).lower()\n",
    "            href = link['href']\n",
    "\n",
    "            if is_relevant_link(link_text):\n",
    "                # Resolve absolute URL\n",
    "                agreement_url = urljoin(url, href)\n",
    "                print(f\" - Found link: {link_text} → {agreement_url}\")\n",
    "\n",
    "                try:\n",
    "                    # Fetch the agreement page\n",
    "                    agreement_response = requests.get(agreement_url, headers=HEADERS, timeout=10)\n",
    "                    if agreement_response.status_code != 200:\n",
    "                        print(f\"   - Failed to fetch: HTTP {agreement_response.status_code}\")\n",
    "                        continue\n",
    "\n",
    "                    agreement_soup = BeautifulSoup(agreement_response.text, 'html.parser')\n",
    "                    content_tags = agreement_soup.find_all(['div', 'section', 'article', 'main'])\n",
    "\n",
    "                    # Extract the largest non-empty block of text\n",
    "                    content_texts = [\n",
    "                        tag.get_text(separator=\" \", strip=True) for tag in content_tags\n",
    "                        if tag.get_text(strip=True)\n",
    "                    ]\n",
    "                    if not content_texts:\n",
    "                        continue\n",
    "\n",
    "                    # Choose the longest block of text (likely the full agreement)\n",
    "                    main_text = max(content_texts, key=len)\n",
    "\n",
    "                    filename = f\"agreements/{file_index:04d}.txt\"\n",
    "                    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "                        f.write(main_text)\n",
    "\n",
    "                    print(f\"   - Saved to {filename}\")\n",
    "                    file_index += 1\n",
    "                    found = True\n",
    "                    break  # Stop after one valid link found\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"   - Error fetching linked page: {e}\")\n",
    "                    continue\n",
    "\n",
    "        if not found:\n",
    "            print(\" - No relevant agreement links found.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\" - Error accessing {url}: {e}\")\n",
    "\n",
    "    time.sleep(1)  # polite pause\n",
    "\n",
    "print(\"\\n Finished all sites.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spider",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
